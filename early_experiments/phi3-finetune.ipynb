{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export CUDA_VISIBLE_DEVICES=0,1\n",
    "#jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import clean_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch; torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mainuser/anaconda3/envs/sqlft/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-23 13:41:21,859] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mainuser/anaconda3/envs/sqlft/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 13.28it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# This command is run in a bash shell due to '%%bash' at the beginning.\n",
    "# 'pip -q install' is used to install Python packages with pip, Python's package installer, in a quiet mode which reduces the output verbosity.\n",
    "# 'huggingface_hub', 'transformers', 'peft', and 'bitsandbytes' are the packages being installed by the first command.\n",
    "# These packages are necessary for the fine-tuning and inference of the Phi-3 model.\n",
    "# 'trl' and 'xformers' are additional packages being installed by the second command.\n",
    "# 'datasets' is a package for providing access to a vast range of datasets, installed by the third command.\n",
    "# The last command ensures that 'torch' version is at least 1.10. If it's already installed but the version is lower, it will be upgraded.\n",
    "# %%bash\n",
    "# pip -q install huggingface_hub transformers peft bitsandbytes\n",
    "# pip -q install trl xformers\n",
    "# pip -q install datasets\n",
    "# pip install torch>=1.10\n",
    "     \n",
    "\n",
    "# Import necessary modules from the transformers library\n",
    "# AutoModelForCausalLM: This is a class for causal language models. It's used for tasks like text generation.\n",
    "# AutoTokenizer: This class is used for tokenizing input data, a necessary step before feeding data into a model.\n",
    "# TrainingArguments: This class is used for defining the parameters for model training, like learning rate, batch size, etc.\n",
    "# BitsAndBytesConfig: This class is used for configuring the BitsAndBytes quantization process.\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "\n",
    "# Import necessary modules from the huggingface_hub library\n",
    "# ModelCard: This class is used for creating a model card, which provides information about a model.\n",
    "# ModelCardData: This class is used for defining the data of a model card.\n",
    "# HfApi: This class provides an interface to the Hugging Face API, allowing you to interact with the Hugging Face Model Hub.\n",
    "from huggingface_hub import ModelCard, ModelCardData, HfApi\n",
    "\n",
    "# Import the load_dataset function from the datasets library. This function is used for loading datasets.\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Import the Template class from the jinja2 library. This class is used for creating dynamic HTML templates.\n",
    "from jinja2 import Template\n",
    "\n",
    "# Import the SFTTrainer class from the trl library. This class is used for training models.\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Import the yaml module. This module is used for working with YAML files.\n",
    "import yaml\n",
    "\n",
    "# Import the torch library. This library provides tools for training and running deep learning models.\n",
    "import torch\n",
    "     \n",
    "\n",
    "# MODEL_ID is a string that specifies the identifier of the pre-trained model that will be fine-tuned. \n",
    "# In this case, the model is 'Phi-3-mini-4k-instruct' from Microsoft.\n",
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# NEW_MODEL_NAME is a string that specifies the name of the new model after fine-tuning.\n",
    "# Here, the new model will be named 'opus-samantha-phi-3-mini-4k'.\n",
    "NEW_MODEL_NAME = \"sql-leetcoder-phi-3-mini-4k\"\n",
    "     \n",
    "\n",
    "# DATASET_NAME is a string that specifies the name of the dataset to be used for fine-tuning.\n",
    "# Replace \"replace with your dataset\" with the actual name of your dataset.\n",
    "DATASET_NAME = \"gretelai/synthetic_text_to_sql\"\n",
    "\n",
    "# SPLIT specifies the portion of the dataset to be used. In this case, the 'train' split of the dataset will be used.\n",
    "SPLIT = \"train\"\n",
    "\n",
    "# MAX_SEQ_LENGTH is an integer that specifies the maximum length of the sequences that the model will handle.\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "# num_train_epochs is an integer that specifies the number of times the training process will go through the entire dataset.\n",
    "num_train_epochs = 1\n",
    "\n",
    "# license is a string that specifies the license under which the model is distributed. In this case, it's Apache License 2.0.\n",
    "license = \"apache-2.0\"\n",
    "\n",
    "# username is a string that specifies the GitHub username of the person who is fine-tuning the model.\n",
    "username = \"dapopov-st\"\n",
    "\n",
    "# learning_rate is a float that specifies the learning rate to be used during training.\n",
    "learning_rate = 1.41e-5\n",
    "\n",
    "# per_device_train_batch_size is an integer that specifies the number of samples to work through before updating the internal model parameters.\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# gradient_accumulation_steps is an integer that specifies the number of steps to accumulate gradients before performing a backward/update pass.\n",
    "gradient_accumulation_steps = 1\n",
    "     \n",
    "\n",
    "# This code checks if the current CUDA device supports bfloat16 (Brain Floating Point) computations.\n",
    "# If bfloat16 is supported, it sets the compute_dtype to torch.bfloat16.\n",
    "# If not, it sets the compute_dtype to torch.float16.\n",
    "# bfloat16 and float16 are both half-precision floating-point formats, but bfloat16 provides better performance on some hardware.\n",
    "if torch.cuda.is_bf16_supported():\n",
    "  compute_dtype = torch.bfloat16\n",
    "else:\n",
    "  compute_dtype = torch.float16\n",
    "     \n",
    "\n",
    "# Load the pre-trained model specified by MODEL_ID using the AutoModelForCausalLM class.\n",
    "# The 'trust_remote_code=True' argument allows the execution of code from the model card (if any).\n",
    "#model = AutoModelForCausalLM.from_pretrained(MODEL_ID, trust_remote_code=True).save_pretrained('./phi3')\n",
    "model = AutoModelForCausalLM.from_pretrained('./phi3')\n",
    "#model = AutoModelForCausalLM.from_pretrained('./phi3')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./phi3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Tell me about yourself'\n",
    "inputs = tokenizer(prompt,\n",
    "                   return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate a response from the model\n",
    "# do_sample=True means the model will generate text by sampling from the distribution of possible outputs\n",
    "# max_new_tokens=120 limits the length of the generated text to 120 tokens\n",
    "outputs = model.generate(**inputs,\n",
    "                         do_sample=True, max_new_tokens=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me about yourself, Jack.\"\n",
      "\n",
      "Jack looked down at his hands, fidgeting with his wedding ring. He took a deep breath and began, \"Well, I'm Jack Turner, first-generation American. My parents, Jameson and Ella Turner, were both immigrants from Ireland. My father worked as a carpenter in the bustling shipyards during the Great Depression, and my mother was a seamstress in a local factory. Despite the financial hardships they faced, they worked tirelessly to give their children a better life.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "     \n",
    "\n",
    "# Print the generated response from the model\n",
    "print(response)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "#dataset.save_to_disk('./data')\n",
    "dataset = load_from_disk('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation'],\n",
       "    num_rows: 100000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the tokenizer associated with the pre-trained model specified by MODEL_ID using the AutoTokenizer class.\n",
    "# The 'trust_remote_code=True' argument allows the execution of code from the model card (if any).\n",
    "#tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "# Load the dataset specified by DATASET_NAME using the load_dataset function.\n",
    "# The 'split=\"train\"' argument specifies that we want to load the training split of the dataset.\n",
    "#dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "\n",
    "# Get the ID of the end-of-sentence (EOS) token from the tokenizer and store it in EOS_TOKEN.\n",
    "# This token is used to mark the end of a sentence in the input data.\n",
    "EOS_TOKEN=tokenizer.eos_token_id\n",
    "     \n",
    "\n",
    "# This line simply prints the contents of the 'dataset' variable.\n",
    "# 'dataset' is expected to be a Dataset object loaded from the 'datasets' library.\n",
    "# Printing it will display information about the dataset such as the number of samples, the features, and a few example data points.\n",
    "dataset\n",
    "     \n",
    "\n",
    "# Select a subset of the data for faster processing\n",
    "#dataset = dataset.select(range(100))\n",
    "     \n",
    "\n",
    "# This line simply prints the contents of the 'dataset' variable.\n",
    "# 'dataset' is expected to be a Dataset object loaded from the 'datasets' library.\n",
    "# Printing it will display information about the dataset such as the number of samples, the features, and a few example data points.\n",
    "#dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer\n",
    "#tokenizer.save_pretrained('./phi3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_func(example):\n",
    "    # Replace 'column_name' and 'value' with the actual column name and value\n",
    "    return (example['sql_complexity'] == 'window functions' \n",
    "            or example['sql_complexity'] == 'multiple_joins'\n",
    "            or example['sql_complexity'] == 'subqueries'\n",
    "            )\n",
    "dataset = dataset.filter(filter_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation'],\n",
       "    num_rows: 13264\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def formatting_prompts_func(sample):\n",
    "    \"\"\"Given a sample dictionary with keys \"title\" and \"abstract\" format into a prompt.\n",
    "\n",
    "    Args:\n",
    "      sample: A sample dictionary from a Hugging Face dataset.\n",
    "\n",
    "    Returns:\n",
    "      sample: sample dictionary with \"text\" key for the formatted prompt.\n",
    "    \"\"\"\n",
    "    #sample['text']=f\"[INST] <> Write SQL code to answer the question based on the context. Please wrap your code answer using ```: <> {sample['question']} {sample['context']} [/INST] {sample['answer']}\"\n",
    "    sample['text']=f\"\"\"\n",
    "<|system|>\n",
    "You are a helpful assistant who is a SQL expert. Your task is to write a SQL query based on the given context and question.<|end|>\n",
    "<|user|>\n",
    "Context: {sample['sql_context']}\n",
    "Question: {sample['sql_prompt']}<|end|>\n",
    "<|assistant|>\n",
    "SQL Query:\n",
    "{sample['sql']}\n",
    "<|end|>\n",
    "\"\"\"\n",
    "    return sample\n",
    "    #return {\"text\":sample['text']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|system|>\n",
      "You are a helpful assistant who is a SQL expert. Your task is to write a SQL query based on the given context and question.<|end|>\n",
      "<|user|>\n",
      "Context: CREATE TABLE cotton_source (brand VARCHAR(255), country VARCHAR(255), quantity INT); INSERT INTO cotton_source (brand, country, quantity) VALUES ('BrandA', 'USA', 1500), ('BrandB', 'USA', 2000), ('BrandC', 'China', 1000);\n",
      "Question: What is the total quantity of cotton sourced from the United States by brands that have committed to fair labor practices?<|end|>\n",
      "<|assistant|>\n",
      "SQL Query:\n",
      "SELECT SUM(quantity) FROM cotton_source WHERE country = 'USA' AND brand IN (SELECT brand FROM fair_labor WHERE commitment = 'yes');\n",
      "<|end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "     \n",
    "\n",
    "# Define a function to format the prompts in the dataset.\n",
    "# This function takes a batch of examples and returns a dictionary with the key 'text' and the value being a list of formatted texts.\n",
    "# def formatting_prompts_func(examples):\n",
    "#     # Extract the conversations from the examples.\n",
    "#     convos = examples[\"conversations\"]\n",
    "#     # Initialize an empty list to store the formatted texts.\n",
    "#     texts = []\n",
    "#     # Define a dictionary to map the 'from' field in the conversation to a prefix.\n",
    "#     mapper = {\"system\": \"system\\n\", \"human\": \"\\nuser\\n\", \"gpt\": \"\\nassistant\\n\"}\n",
    "#     # Define a dictionary to map the 'from' field in the conversation to a suffix.\n",
    "#     end_mapper = {\"system\": \"\", \"human\": \"\", \"gpt\": \"\"}\n",
    "#     # Iterate over each conversation.\n",
    "#     for convo in convos:\n",
    "#         # Format the conversation by joining each turn with its corresponding prefix and suffix.\n",
    "#         # Append the EOS token to the end of the conversation.\n",
    "#         text = \"\".join(f\"{mapper[(turn := x['from'])]} {x['value']}\\n{end_mapper[turn]}\" for x in convo)\n",
    "#         texts.append(f\"{text}{EOS_TOKEN}\")\n",
    "#     # Return the formatted texts.\n",
    "#     return {\"text\": texts}\n",
    "\n",
    "# Apply the formatting function to the dataset using the map method.\n",
    "# The 'batched=True' argument means that the function is applied to batches of examples.\n",
    "dataset = dataset.map(formatting_prompts_func,\n",
    "                      remove_columns=['id','domain','domain_description',\n",
    "                                      'sql_complexity','sql_complexity_description',\n",
    "                                      'sql_task_type','sql_task_type_description',\n",
    "                                      'sql_explanation','sql_prompt', 'sql_context', 'sql',])#, batched=True)\n",
    "\n",
    "# Print the 9th example from the 'text' field of the dataset to check the result.\n",
    "print(dataset['text'][8])\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 13264\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import LoraConfig\n",
    "# peft_config = LoraConfig(\n",
    "#       lora_alpha=16,\n",
    "#       lora_dropout=0.1,\n",
    "#       r=64,\n",
    "#       bias=\"none\",\n",
    "#       task_type=\"CAUSAL_LM\",\n",
    "#       use_dora=True,\n",
    "#       target_modules=[\n",
    "#         \"model.layers.*.self_attn.qkv_proj.*\",\n",
    "#         \"model.layers.*.self_attn.o_proj.*\",\n",
    "#         \"model.layers.*.mlp.gate_up_proj.*\",\n",
    "#         \"model.layers.*.mlp.down_proj.*\",\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "peft_config = LoraConfig(\n",
    "      lora_alpha=16,\n",
    "      lora_dropout=0.1,\n",
    "      r=64,\n",
    "      bias=\"none\",\n",
    "      task_type=\"CAUSAL_LM\",\n",
    "      use_dora=True,\n",
    "      target_modules=[\n",
    "        \"qkv_proj\",\n",
    "        \"o_proj\",\n",
    "        \"mlp.gate_up_proj\",\n",
    "        \"mlp.down_proj\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "model.layers.0.self_attn.qkv_proj.weight\n",
      "model.layers.0.mlp.gate_up_proj.weight\n",
      "model.layers.0.mlp.down_proj.weight\n",
      "model.layers.0.input_layernorm.weight\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "model.layers.1.self_attn.o_proj.weight\n",
      "model.layers.1.self_attn.qkv_proj.weight\n",
      "model.layers.1.mlp.gate_up_proj.weight\n",
      "model.layers.1.mlp.down_proj.weight\n",
      "model.layers.1.input_layernorm.weight\n",
      "model.layers.1.post_attention_layernorm.weight\n",
      "model.layers.2.self_attn.o_proj.weight\n",
      "model.layers.2.self_attn.qkv_proj.weight\n",
      "model.layers.2.mlp.gate_up_proj.weight\n",
      "model.layers.2.mlp.down_proj.weight\n",
      "model.layers.2.input_layernorm.weight\n",
      "model.layers.2.post_attention_layernorm.weight\n",
      "model.layers.3.self_attn.o_proj.weight\n",
      "model.layers.3.self_attn.qkv_proj.weight\n",
      "model.layers.3.mlp.gate_up_proj.weight\n",
      "model.layers.3.mlp.down_proj.weight\n",
      "model.layers.3.input_layernorm.weight\n",
      "model.layers.3.post_attention_layernorm.weight\n",
      "model.layers.4.self_attn.o_proj.weight\n",
      "model.layers.4.self_attn.qkv_proj.weight\n",
      "model.layers.4.mlp.gate_up_proj.weight\n",
      "model.layers.4.mlp.down_proj.weight\n",
      "model.layers.4.input_layernorm.weight\n",
      "model.layers.4.post_attention_layernorm.weight\n",
      "model.layers.5.self_attn.o_proj.weight\n",
      "model.layers.5.self_attn.qkv_proj.weight\n",
      "model.layers.5.mlp.gate_up_proj.weight\n",
      "model.layers.5.mlp.down_proj.weight\n",
      "model.layers.5.input_layernorm.weight\n",
      "model.layers.5.post_attention_layernorm.weight\n",
      "model.layers.6.self_attn.o_proj.weight\n",
      "model.layers.6.self_attn.qkv_proj.weight\n",
      "model.layers.6.mlp.gate_up_proj.weight\n",
      "model.layers.6.mlp.down_proj.weight\n",
      "model.layers.6.input_layernorm.weight\n",
      "model.layers.6.post_attention_layernorm.weight\n",
      "model.layers.7.self_attn.o_proj.weight\n",
      "model.layers.7.self_attn.qkv_proj.weight\n",
      "model.layers.7.mlp.gate_up_proj.weight\n",
      "model.layers.7.mlp.down_proj.weight\n",
      "model.layers.7.input_layernorm.weight\n",
      "model.layers.7.post_attention_layernorm.weight\n",
      "model.layers.8.self_attn.o_proj.weight\n",
      "model.layers.8.self_attn.qkv_proj.weight\n",
      "model.layers.8.mlp.gate_up_proj.weight\n",
      "model.layers.8.mlp.down_proj.weight\n",
      "model.layers.8.input_layernorm.weight\n",
      "model.layers.8.post_attention_layernorm.weight\n",
      "model.layers.9.self_attn.o_proj.weight\n",
      "model.layers.9.self_attn.qkv_proj.weight\n",
      "model.layers.9.mlp.gate_up_proj.weight\n",
      "model.layers.9.mlp.down_proj.weight\n",
      "model.layers.9.input_layernorm.weight\n",
      "model.layers.9.post_attention_layernorm.weight\n",
      "model.layers.10.self_attn.o_proj.weight\n",
      "model.layers.10.self_attn.qkv_proj.weight\n",
      "model.layers.10.mlp.gate_up_proj.weight\n",
      "model.layers.10.mlp.down_proj.weight\n",
      "model.layers.10.input_layernorm.weight\n",
      "model.layers.10.post_attention_layernorm.weight\n",
      "model.layers.11.self_attn.o_proj.weight\n",
      "model.layers.11.self_attn.qkv_proj.weight\n",
      "model.layers.11.mlp.gate_up_proj.weight\n",
      "model.layers.11.mlp.down_proj.weight\n",
      "model.layers.11.input_layernorm.weight\n",
      "model.layers.11.post_attention_layernorm.weight\n",
      "model.layers.12.self_attn.o_proj.weight\n",
      "model.layers.12.self_attn.qkv_proj.weight\n",
      "model.layers.12.mlp.gate_up_proj.weight\n",
      "model.layers.12.mlp.down_proj.weight\n",
      "model.layers.12.input_layernorm.weight\n",
      "model.layers.12.post_attention_layernorm.weight\n",
      "model.layers.13.self_attn.o_proj.weight\n",
      "model.layers.13.self_attn.qkv_proj.weight\n",
      "model.layers.13.mlp.gate_up_proj.weight\n",
      "model.layers.13.mlp.down_proj.weight\n",
      "model.layers.13.input_layernorm.weight\n",
      "model.layers.13.post_attention_layernorm.weight\n",
      "model.layers.14.self_attn.o_proj.weight\n",
      "model.layers.14.self_attn.qkv_proj.weight\n",
      "model.layers.14.mlp.gate_up_proj.weight\n",
      "model.layers.14.mlp.down_proj.weight\n",
      "model.layers.14.input_layernorm.weight\n",
      "model.layers.14.post_attention_layernorm.weight\n",
      "model.layers.15.self_attn.o_proj.weight\n",
      "model.layers.15.self_attn.qkv_proj.weight\n",
      "model.layers.15.mlp.gate_up_proj.weight\n",
      "model.layers.15.mlp.down_proj.weight\n",
      "model.layers.15.input_layernorm.weight\n",
      "model.layers.15.post_attention_layernorm.weight\n",
      "model.layers.16.self_attn.o_proj.weight\n",
      "model.layers.16.self_attn.qkv_proj.weight\n",
      "model.layers.16.mlp.gate_up_proj.weight\n",
      "model.layers.16.mlp.down_proj.weight\n",
      "model.layers.16.input_layernorm.weight\n",
      "model.layers.16.post_attention_layernorm.weight\n",
      "model.layers.17.self_attn.o_proj.weight\n",
      "model.layers.17.self_attn.qkv_proj.weight\n",
      "model.layers.17.mlp.gate_up_proj.weight\n",
      "model.layers.17.mlp.down_proj.weight\n",
      "model.layers.17.input_layernorm.weight\n",
      "model.layers.17.post_attention_layernorm.weight\n",
      "model.layers.18.self_attn.o_proj.weight\n",
      "model.layers.18.self_attn.qkv_proj.weight\n",
      "model.layers.18.mlp.gate_up_proj.weight\n",
      "model.layers.18.mlp.down_proj.weight\n",
      "model.layers.18.input_layernorm.weight\n",
      "model.layers.18.post_attention_layernorm.weight\n",
      "model.layers.19.self_attn.o_proj.weight\n",
      "model.layers.19.self_attn.qkv_proj.weight\n",
      "model.layers.19.mlp.gate_up_proj.weight\n",
      "model.layers.19.mlp.down_proj.weight\n",
      "model.layers.19.input_layernorm.weight\n",
      "model.layers.19.post_attention_layernorm.weight\n",
      "model.layers.20.self_attn.o_proj.weight\n",
      "model.layers.20.self_attn.qkv_proj.weight\n",
      "model.layers.20.mlp.gate_up_proj.weight\n",
      "model.layers.20.mlp.down_proj.weight\n",
      "model.layers.20.input_layernorm.weight\n",
      "model.layers.20.post_attention_layernorm.weight\n",
      "model.layers.21.self_attn.o_proj.weight\n",
      "model.layers.21.self_attn.qkv_proj.weight\n",
      "model.layers.21.mlp.gate_up_proj.weight\n",
      "model.layers.21.mlp.down_proj.weight\n",
      "model.layers.21.input_layernorm.weight\n",
      "model.layers.21.post_attention_layernorm.weight\n",
      "model.layers.22.self_attn.o_proj.weight\n",
      "model.layers.22.self_attn.qkv_proj.weight\n",
      "model.layers.22.mlp.gate_up_proj.weight\n",
      "model.layers.22.mlp.down_proj.weight\n",
      "model.layers.22.input_layernorm.weight\n",
      "model.layers.22.post_attention_layernorm.weight\n",
      "model.layers.23.self_attn.o_proj.weight\n",
      "model.layers.23.self_attn.qkv_proj.weight\n",
      "model.layers.23.mlp.gate_up_proj.weight\n",
      "model.layers.23.mlp.down_proj.weight\n",
      "model.layers.23.input_layernorm.weight\n",
      "model.layers.23.post_attention_layernorm.weight\n",
      "model.layers.24.self_attn.o_proj.weight\n",
      "model.layers.24.self_attn.qkv_proj.weight\n",
      "model.layers.24.mlp.gate_up_proj.weight\n",
      "model.layers.24.mlp.down_proj.weight\n",
      "model.layers.24.input_layernorm.weight\n",
      "model.layers.24.post_attention_layernorm.weight\n",
      "model.layers.25.self_attn.o_proj.weight\n",
      "model.layers.25.self_attn.qkv_proj.weight\n",
      "model.layers.25.mlp.gate_up_proj.weight\n",
      "model.layers.25.mlp.down_proj.weight\n",
      "model.layers.25.input_layernorm.weight\n",
      "model.layers.25.post_attention_layernorm.weight\n",
      "model.layers.26.self_attn.o_proj.weight\n",
      "model.layers.26.self_attn.qkv_proj.weight\n",
      "model.layers.26.mlp.gate_up_proj.weight\n",
      "model.layers.26.mlp.down_proj.weight\n",
      "model.layers.26.input_layernorm.weight\n",
      "model.layers.26.post_attention_layernorm.weight\n",
      "model.layers.27.self_attn.o_proj.weight\n",
      "model.layers.27.self_attn.qkv_proj.weight\n",
      "model.layers.27.mlp.gate_up_proj.weight\n",
      "model.layers.27.mlp.down_proj.weight\n",
      "model.layers.27.input_layernorm.weight\n",
      "model.layers.27.post_attention_layernorm.weight\n",
      "model.layers.28.self_attn.o_proj.weight\n",
      "model.layers.28.self_attn.qkv_proj.weight\n",
      "model.layers.28.mlp.gate_up_proj.weight\n",
      "model.layers.28.mlp.down_proj.weight\n",
      "model.layers.28.input_layernorm.weight\n",
      "model.layers.28.post_attention_layernorm.weight\n",
      "model.layers.29.self_attn.o_proj.weight\n",
      "model.layers.29.self_attn.qkv_proj.weight\n",
      "model.layers.29.mlp.gate_up_proj.weight\n",
      "model.layers.29.mlp.down_proj.weight\n",
      "model.layers.29.input_layernorm.weight\n",
      "model.layers.29.post_attention_layernorm.weight\n",
      "model.layers.30.self_attn.o_proj.weight\n",
      "model.layers.30.self_attn.qkv_proj.weight\n",
      "model.layers.30.mlp.gate_up_proj.weight\n",
      "model.layers.30.mlp.down_proj.weight\n",
      "model.layers.30.input_layernorm.weight\n",
      "model.layers.30.post_attention_layernorm.weight\n",
      "model.layers.31.self_attn.o_proj.weight\n",
      "model.layers.31.self_attn.qkv_proj.weight\n",
      "model.layers.31.mlp.gate_up_proj.weight\n",
      "model.layers.31.mlp.down_proj.weight\n",
      "model.layers.31.input_layernorm.weight\n",
      "model.layers.31.post_attention_layernorm.weight\n",
      "model.norm.weight\n",
      "lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "for name in model.state_dict():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "# Create a TrainingArguments object, which is used to define the parameters for model training.\n",
    "\n",
    "args = TrainingArguments(\n",
    "    # 'evaluation_strategy' is set to \"steps\", which means evaluation is done at each logging step.\n",
    "    evaluation_strategy=\"steps\",\n",
    "\n",
    "    # 'per_device_train_batch_size' is set to 7, which means each training batch will contain 7 samples per device.\n",
    "    per_device_train_batch_size=2, #7,\n",
    "\n",
    "    # 'gradient_accumulation_steps' is set to 4, which means gradients are accumulated for 4 steps before performing a backward/update pass.\n",
    "    gradient_accumulation_steps=16,\n",
    "\n",
    "    # 'gradient_checkpointing' is set to True, which means model gradients are stored in memory during training to reduce memory usage.\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    "    # 'learning_rate' is set to 1e-4, which is the learning rate for the optimizer.\n",
    "    learning_rate=1e-4,\n",
    "\n",
    "    # 'fp16' is set to True if bfloat16 is not supported, which means the model will use 16-bit floating point precision for training if possible.\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "\n",
    "    # 'bf16' is set to True if bfloat16 is supported, which means the model will use bfloat16 precision for training if possible.\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "\n",
    "    # 'max_steps' is set to -1, which means there is no maximum number of training steps.\n",
    "    max_steps=10, #-1\n",
    "\n",
    "    # 'num_train_epochs' is set to 3, which means the training process will go through the entire dataset 3 times.\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    # 'save_strategy' is set to \"epoch\", which means the model is saved at the end of each epoch.\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    # 'logging_steps' is set to 10, which means logging is done every 10 steps.\n",
    "    logging_steps=10,\n",
    "\n",
    "    # 'output_dir' is set to NEW_MODEL_NAME, which is the directory where the model and its configuration will be saved.\n",
    "    output_dir=NEW_MODEL_NAME,\n",
    "\n",
    "    # 'optim' is set to \"paged_adamw_32bit\", which is the optimizer to be used for training.\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "\n",
    "    # 'lr_scheduler_type' is set to \"linear\", which means the learning rate scheduler type is linear.\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    \n",
    ")\n",
    "     \n",
    "\n",
    "# Create an instance of the SFTTrainer class, which is used to fine-tune the model.\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    # 'model' is the pre-trained model that will be fine-tuned.\n",
    "    model=model,\n",
    "\n",
    "    # 'args' are the training arguments that specify the training parameters.\n",
    "    args=args,\n",
    "\n",
    "    # 'train_dataset' is the dataset that will be used for training.\n",
    "    train_dataset=dataset,\n",
    "\n",
    "    # 'dataset_text_field' is the key in the dataset that contains the text data.\n",
    "    dataset_text_field=\"text\",\n",
    "\n",
    "    # 'max_seq_length' is the maximum length of the sequences that the model will handle.\n",
    "    max_seq_length=128,\n",
    "\n",
    "    # 'formatting_func' is the function that will be used to format the prompts in the dataset.\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    peft_config=peft_config,\n",
    "    \n",
    ")\n",
    "     \n",
    "\n",
    "# 'device' is set to 'cuda', which means the CUDA device will be used for computations if available.\n",
    "device = 'cuda'\n",
    "\n",
    "# Import the 'gc' module, which provides an interface to the garbage collector.\n",
    "import gc\n",
    "\n",
    "# Import the 'os' module, which provides a way of using operating system dependent functionality.\n",
    "import os\n",
    "\n",
    "# Call the 'collect' method of the 'gc' module to start a garbage collection, which can help free up memory.\n",
    "gc.collect()\n",
    "\n",
    "# Call the 'empty_cache' method of 'torch.cuda' to release all unused cached memory from PyTorch so that it can be used by other GPU applications.\n",
    "torch.cuda.empty_cache()\n",
    "     \n",
    "\n",
    "# Call the 'train' method of the 'trainer' object to start the training process.\n",
    "# This method will fine-tune the model on the training dataset according to the parameters specified in the 'args' object.\n",
    "trainer.train()\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sqlft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
